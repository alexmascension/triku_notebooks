{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing other feature selection methods with triku\n",
    "In this notebook we will compare the performance of triku, compared to other methods. \n",
    "\n",
    "The methods that will be compared will be the following:\n",
    "* Select genes with highest variance.   \n",
    "* Scanpy's `sc.pp.highly_variable_genes`: It is based on Seurat's `vst` method, so they should return similar results.\n",
    "* scry `devianceFeatureSelection()`. This method is featured as the feature selection for Irizarry's GLM-PCA paper (https://doi.org/10.1186/s13059-019-1861-6). From its description, it computes a deviance statistic for each row feature for count data based on a multinomial null model that assumes each feature has a constant rate. Features with large deviance are likely to be informative. Uninformative, low deviance features can be discarded to speed up downstream analyses and reduce memory footprint. The `fam`parameter will be set to `binomial`, the default.\n",
    "* M3Drop, which has two main functions:\n",
    "    * NBDrop: the NBDrop model assumes proportion of zeros follows a Michaelis-Menten model. Then the Michaelis-Menten parameter $K$ is fitted. For each gene, its parameter $K_i$ is compared to $K$ using a $Z$-test, which returns the selected genes.\n",
    "    * NBUmi: The procedure is similar to above, although the equation to fit now is a negative binomial model,  and the selection of genes is then done using a $Z$-test.\n",
    "* `BrenneckeGetVariableGenes` fits a function between CV$^2$ and mean expression. \n",
    "* Seurat's `FindVariableFeatures`\n",
    "* SCTransform\n",
    "\n",
    "With the exception of scanpy and triku, the rest of functions are set on $R$. We will use jupyter's `%%R` magic command, and `anndata2ri` to transform `annData` into `SingleCellExperiment` objects, and we will generate the functions to accept that annData and return the list of selected features. The functions have to be set up in notebook, and cannot be externalized. \n",
    "\n",
    "M3Drop requires a normalization step, which will be done in-situ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triku as tk\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as spr\n",
    "import scipy.stats as sts\n",
    "import os\n",
    "import gc\n",
    "from itertools import product\n",
    "import pickle\n",
    "import ray\n",
    "import seaborn as sns\n",
    "import itertools \n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from bokeh.io import show, output_notebook, reset_output\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import LinearColorMapper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score as ARI\n",
    "from sklearn.metrics import adjusted_mutual_info_score as NMI\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "reset_output()\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_methods = ['triku', 'm3drop', 'nbumi', 'scanpy', 'seurat', 'sct', 'scry', 'std', 'brennecke',]\n",
    "list_methods_all = list_methods + ['all', 'random']\n",
    "\n",
    "palette = [\n",
    "        '#e91e63',  # triku\n",
    "        '#81c784',  # m3drop\n",
    "        '#388e3c',  # nbumi\n",
    "        '#90caf9',  # scanpy\n",
    "        '#2196f3',  # seurat\n",
    "        '#1565c0',  # sctransform\n",
    "        '#ff9800',  # std\n",
    "        '#ff5722',  # scry\n",
    "        '#ffca28',  # brennecke\n",
    "]\n",
    "\n",
    "palette_all =  palette +  [ \n",
    "        '#A5B1C2',  # all\n",
    "        '#4B6584',  # random\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.getcwd() + '/code')\n",
    "\n",
    "from triku_nb_code.comparing_feat_sel import plot_max_var_x_dataset, plot_max_var_x_method, create_dict_UMAPs_datasets, \\\n",
    "get_max_diff_gene, plot_ARI_x_method, plot_ARI_x_dataset, biological_silhouette_ARI_table, plot_lab_org_comparison_scores, \\\n",
    "clustering_binary_search, compare_rankings, compare_values\n",
    "from triku_nb_code.comparing_feat_sel import create_UMAP_adataset_libprep_org, plot_UMAPs_datasets, plot_XY, biological_silhouette_ARI_table\n",
    "from triku_nb_code.palettes_and_cmaps import magma, bold_and_vivid, prism\n",
    "from triku_nb_code.GOEA_figs import scatter_enrichr, barplot_ontologies_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata2ri\n",
    "anndata2ri.activate()\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Load all the R libraries we will be using in the notebook\n",
    "library(M3Drop) # Depends on r-foreing (conda-forge) and Hmisc and reldist (install.packages)\n",
    "library(scry) # If R < 4, launch commit 9f0fc819\n",
    "library(Seurat)\n",
    "library(sctransform)\n",
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.getcwd() + '/exports/comparisons/', exist_ok=True)\n",
    "os.makedirs(os.getcwd() + '/figures/comparison_figs/png', exist_ok=True)\n",
    "os.makedirs(os.getcwd() + '/figures/comparison_figs/pdf', exist_ok=True)\n",
    "data_dir = os.getcwd() + '/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following 3 cells we will create the cells that obtain the most relevant features. Since some of the calls are to R, they have to be kept as separate cells. Also, we create the function `create_df_feature_ranking` which creates two dataframes: one with the evaluation values (p-value, emd distance, etc.) of each method, and the second one with the ranking of genes based on those values. These dataframes will be valuable so that we don't have to repeat the calling to the feature selection methods each time we do a graph. `create_df_feature_ranking` is also kept as a cell because it makes some calls to R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "run_scry <- function(sce){ #adata\n",
    "    adata_ret = devianceFeatureSelection(sce, nkeep=dim(sce)[1], assay='X')\n",
    "    return(adata_ret) #returns adata with stats on .var\n",
    "} \n",
    "\n",
    "\n",
    "run_brennecke <- function(sce){ #df\n",
    "    res_df <- BrenneckeGetVariableGenes(sce, suppress.plot=TRUE, fdr=100)\n",
    "    return(res_df) # returns sorted df with genes and stats\n",
    "}\n",
    "\n",
    "\n",
    "run_M3Drop <- function(sce){\n",
    "    norm <- M3DropConvertData(sce, is.counts=TRUE)\n",
    "    DE_genes <- M3DropFeatureSelection(norm, suppress.plot=TRUE, mt_threshold=50)\n",
    "    return(DE_genes) # returns sorted df with genes and stats\n",
    "    \n",
    "}\n",
    "\n",
    "run_NBumi <- function(sce){\n",
    "    count_mat <- NBumiConvertData(sce, is.counts=TRUE)\n",
    "    DANB_fit <- NBumiFitModel(count_mat)\n",
    "    NBDropFS <- NBumiFeatureSelectionCombinedDrop(DANB_fit, suppress.plot=TRUE, qval.thresh=10)\n",
    "    return(NBDropFS)  # returns sorted df with genes and stats\n",
    "    \n",
    "}\n",
    "\n",
    "run_seurat <- function(sce){ #adata\n",
    "    sce <- FindVariableFeatures(sce, selection.method = \"vst\", nfeatures = dim(sce)[1])\n",
    "    index <- c(1:dim(sce)[1])\n",
    "    names <- VariableFeatures(sce)\n",
    "    df_seurat <- HVFInfo(sce)[VariableFeatures(sce), ]\n",
    "    return(df_seurat)\n",
    "} \n",
    "\n",
    "\n",
    "run_sct <- function(sce){ #adata\n",
    "    sce <- SCTransform(sce, verbose = FALSE)\n",
    "    df_sct <- sce@assays$SCT@SCTModel.list$model1@feature.attributes\n",
    "    return(df_sct)\n",
    "} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scanpy(adata):\n",
    "    adata_copy = adata.copy()\n",
    "    if not 'log1p' in adata_copy.uns:\n",
    "        sc.pp.log1p(adata_copy)\n",
    "    ret = sc.pp.highly_variable_genes(adata_copy, n_top_genes=len(adata_copy), inplace=False)\n",
    "    df = pd.DataFrame(ret)\n",
    "    df =  df.set_index(adata_copy.var_names)\n",
    "    del adata_copy; gc.collect()\n",
    "    return df # returns df with stats\n",
    "\n",
    "def run_variable(adata):\n",
    "    if spr.issparse(adata.X):\n",
    "        std = adata.X.power(2).mean(0) - np.power(adata.X.mean(0), 2) \n",
    "        std = np.asarray(std).flatten()        \n",
    "    else:\n",
    "        std = adata.X.std(0)\n",
    "        \n",
    "    return std #returns vector with order as var_names \n",
    "\n",
    "def run_triku(adata, seed, dist_conn='conn'):\n",
    "    adata_copy = adata.copy()\n",
    "    # We have seen that whitening the matrix (not only zero-centring) yielded the best results. This is not directly applycable \n",
    "    try:\n",
    "        pca = PCA(n_components=30, whiten=True, svd_solver=\"auto\", random_state=seed,).fit_transform(adata_copy.X.toarray())\n",
    "    except: # the array is already dense\n",
    "        pca = PCA(n_components=30, whiten=True, svd_solver=\"auto\", random_state=seed,).fit_transform(adata_copy.X)\n",
    "        \n",
    "    adata_copy.obsm['X_pca'] = pca\n",
    "    sc.pp.neighbors(adata_copy, random_state=seed, metric='cosine', n_neighbors=int(len(adata_copy) ** 0.5))\n",
    "    tk.tl.triku(adata_copy, n_windows=100, verbose='error', dist_conn=dist_conn)\n",
    "    \n",
    "    if dist_conn == 'conn':\n",
    "        print('n_HVG: ', adata_copy.var['highly_variable'].sum())\n",
    "    d = adata_copy.var['triku_distance'] #pd series with distance\n",
    "    del adata_copy; gc.collect()\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_feature_ranking(adatax, title_prefix, apply_log=False):\n",
    "    \"\"\"\n",
    "    Create a dataframe with the ranking of features, and another one with the feature values. The adata must be the raw\n",
    "    adata. From that we will create a adata_df necessary for some R methods.\n",
    "    \n",
    "    After each method is run, we will fill the dataframe values, with the values of the metrics used for feature selection, \n",
    "    and the dataframe of rankings with the rankings based on the returned value (0, 1, 2, etc.). \n",
    "    We create two separate dataframes because the df with values might be reserved for other purposes. The rank dataframes is interesting\n",
    "    because the values on the values dataframe have different argsort orders depending on the column (M3drop and NBumi direct, rest reverse).\n",
    "    \"\"\"\n",
    "    \n",
    "    adata = adatax.copy()\n",
    "    sc.pp.filter_genes(adata, min_cells=1) \n",
    "    sc.pp.filter_cells(adata, min_genes=1)\n",
    "    adata.layers['raw'] = adata.X.copy()\n",
    "    \n",
    "    if apply_log:\n",
    "        sc.pp.log1p(adata)\n",
    "    \n",
    "    adata_df = pd.DataFrame(adata.X.T, index=adata.var_names, columns=adata.obs_names)\n",
    "    adata_raw_df = pd.DataFrame(adata.layers['raw'].T, index=adata.var_names, columns=adata.obs_names)\n",
    "    \n",
    "    adata_short = sc.AnnData(X = adata.X[:,:]) # we have to create a clean adata because some column break Rpush\n",
    "    adata_short.var_names, adata_short.obs_names = adata.var_names[:], adata.obs_names[:]\n",
    "\n",
    "    adata_seurat = adata_short.copy()\n",
    "    adata_seurat.obs['nCount_RNA'] = adata_raw_df.values.sum(0).astype(int)\n",
    "    adata_seurat.obs['nFeature_RNA'] = (adata_raw_df.values > 0).sum(0)\n",
    "\n",
    "    %Rpush adata_seurat\n",
    "    %Rpush adata_short\n",
    "    %Rpush adata_df\n",
    "    %Rpush adata_raw_df\n",
    "\n",
    "    %R assay(adata_seurat, \"raw\") <- data.matrix(adata_raw_df)\n",
    "    %R adata_seurat <- as.Seurat(adata_seurat, counts = 'X', data = 'raw')\n",
    "    \n",
    "    print('Outside R', adata.shape, adata_short.shape)\n",
    "    d = %R  dim(adata_short)\n",
    "    print('Inside R', d)\n",
    "       \n",
    "    if 'Group' in adata.obs:\n",
    "        adata_groups = [i.replace('Group', '') for i in adata.obs['Group']]\n",
    "        adata.obs['groupn'] = adata_groups\n",
    "    \n",
    "    index, columns = adata.var_names, ['triku', 'triku_dist', 'scanpy', 'std', 'scry', 'brennecke', 'm3drop', 'nbumi', 'seurat', 'sct']\n",
    "\n",
    "    df_values, df_ranks = pd.DataFrame(index=index, columns=columns), pd.DataFrame(index=index, columns=columns)\n",
    "    \n",
    "    \n",
    "    df_emd_distance = run_triku(adata, seed=0, dist_conn='conn')\n",
    "    df_values.loc[df_emd_distance.index, f'triku'] = df_emd_distance.values\n",
    "\n",
    "    \n",
    "    df_emd_distance = run_triku(adata, seed=0, dist_conn='dist')\n",
    "    df_values.loc[df_emd_distance.index, f'triku_dist'] = df_emd_distance.values\n",
    "    \n",
    "    \n",
    "    scanpy_ret = run_scanpy(adata)\n",
    "    df_values.loc[scanpy_ret.index, 'scanpy'] = scanpy_ret['dispersions_norm'].values\n",
    "    assert len(df_values.index) == len(adata.var_names)\n",
    "    \n",
    "    std_ret = run_variable(adata)\n",
    "    df_values.loc[:, 'std'] = std_ret\n",
    "    assert len(df_values.index) == len(adata.var_names)\n",
    "    \n",
    "    scry_ret = %R run_scry(adata_short)\n",
    "    df_values.loc[scry_ret.var.index, 'scry'] = scry_ret.var['binomial_deviance'].values\n",
    "    assert len(df_values.index) == len(adata.var_names)\n",
    "    \n",
    "    brennecke_ret = %R run_brennecke(adata_df)\n",
    "    df_values.loc[brennecke_ret.index, 'brennecke'] = brennecke_ret['effect.size'].values\n",
    "    assert len(df_values.index) == len(adata.var_names)\n",
    "    \n",
    "    M3Drop_ret = %R run_M3Drop(adata_df)\n",
    "    df_values.loc[M3Drop_ret.index, 'm3drop'] = M3Drop_ret['q.value'].values\n",
    "    assert len(df_values.index) == len(adata.var_names)\n",
    "    \n",
    "    NBumi_ret = %R run_NBumi(adata_df)\n",
    "    df_values.loc[NBumi_ret.index, 'nbumi'] = NBumi_ret['q.value'].values\n",
    "    assert len(df_values.index) == len(adata.var_names)\n",
    "    \n",
    "    seurat_ret = %R run_seurat(adata_seurat)\n",
    "    df_values.loc[seurat_ret.index, 'seurat'] = seurat_ret['variance.standardized'].values\n",
    "    assert len(df_values.index) == len(adata.var_names)\n",
    "    \n",
    "    sct_ret = %R run_sct(adata_seurat)\n",
    "    df_values.loc[sct_ret.index, 'sct'] = sct_ret['residual_variance'].values\n",
    "    assert len(df_values.index) == len(adata.var_names)  \n",
    "\n",
    "    \n",
    "    # Now we will fill df_ranks with an argsort !!!!! M3DROP and NBumi is not [::-1] because they are q-values \n",
    "    for col in columns:\n",
    "        df_ranks[col] = df_values[col].values.argsort()[::-1].argsort()\n",
    "    for col in ['m3drop', 'nbumi']:\n",
    "        df_ranks[col] = df_values[col].values.argsort().argsort() # double argsort to return the rank!\n",
    "    \n",
    "    df_ranks.to_csv(os.getcwd() + '/exports/comparisons/' + title_prefix + '_feature_ranks.csv')\n",
    "    df_values.to_csv(os.getcwd() + '/exports/comparisons/' + title_prefix + '_feature_values.csv')\n",
    "    print('df_ranks', df_ranks.shape)\n",
    "    \n",
    "    del adata; gc.collect()\n",
    "    return df_values, df_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random datasets\n",
    "For this section we will use the random datasets generated with splatter.\n",
    "To evaluate the performance of the feature selection methods, we will use teo metrics, maximum deviation and ARI, explained below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splatter_dir = os.getcwd() + '/data/splatter/'\n",
    "list_deprobs = [0.0065, 0.008, 0.01, 0.016, 0.025, 0.05, 0.1, 0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THIS PROCESS TAKES ~ 4 HOURS!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also... this cell sometimes fails to load. Running it again makes it go fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "triku_logger = tk.logg.triku_logger\n",
    "triku_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for deprob in tqdm(list_deprobs[2:]):\n",
    "    print(f'Deprob {deprob}')\n",
    "    adata_deprob = sc.read(splatter_dir + f'/splatter_deprob_{deprob}.loom', sparse=False)\n",
    "    print(f'Adata {deprob} loaded: {adata_deprob.X.shape}')\n",
    "    df_values, df_ranks = create_df_feature_ranking(adata_deprob, f'scatter_{deprob}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARI / NMI\n",
    "Using ARI on random datasets is a measure to assess the effectiveness of the feature selection. Random datasets were prepared with different degrees of differentially expressed gene probability, so that we can compare the leiden clusterign solution with the 9 populations. Triku can be run with different seeds, but the rest of methods are deterministic. However, leiden clustering in all cases can be run with a seed. Therefore, we are going to run all processes with 10 seeds (although the deterministic processes will be run once).\n",
    "\n",
    "To apply the ARI we need to run leiden with as many clusters as scatter populations. Since leiden runs on resolution, we need to adjust the resolution parameter to match the number of clusters. To do that we are going to implement a binary search-like algorithm. We will start with resolutions 0.3 and 2 (may change in the future). If any of those yields the clusters, done. Else, find the midpoint, run the clustering, and if the clustering yields the number of populations, stop. Else, set the upper or lower resolution to the one that makes the desired number of clusters to be in the middle. This algorithm will try at most 5 times (it gets to resolution differences of ~0.05, which is fair)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the ARI, we need to load a dataset, select a number of features, and create the dataframe with seeds as rows (to see varation on clustering / triku) and the methods as columns. Because creating each dataframe take time (there are 70 cells to be filled), we will choose two datasets (DE = 0.01 and 0.025) and two number of features (100 and 500), which show good results in the previous sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.getcwd() + '/exports/comparisons/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_res, max_res, max_depth = 0.3, 2, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def leiden_adata_NMI_ARI(deprobx):\n",
    "    print(deprobx)\n",
    "    adata_all = sc.read(splatter_dir + f'/splatter_deprob_{deprobx}.loom', sparse=False)\n",
    "    sc.pp.subsample(adata_all, 0.4) # We shorthen this to make the calculations not take 8 hours!\n",
    "    sc.pp.filter_genes(adata_all, min_cells=1)\n",
    "    sc.pp.filter_cells(adata_all, min_genes=1)\n",
    "    sc.pp.log1p(adata_all)\n",
    "    \n",
    "    for n_features in [250, 500]:\n",
    "        print(deprobx, n_features)\n",
    "        if not os.path.exists(os.getcwd() + f'/exports/comparisons/NMI_scatter_{deprobx}_n_features_{n_features}.csv'):\n",
    "            df_feature_ranks = pd.read_csv(os.getcwd() + '/exports/comparisons/' + f'scatter_{deprobx}' + '_feature_ranks.csv', index_col=0)\n",
    "\n",
    "            list_methods = df_feature_ranks.columns.tolist() + ['all', 'random']\n",
    "\n",
    "            df_NMI = pd.DataFrame(index=[f'seed_{i}' for i in range(10)], columns=list_methods)\n",
    "            df_ARI = pd.DataFrame(index=[f'seed_{i}' for i in range(10)], columns=list_methods)\n",
    "\n",
    "            for seed in range(10):\n",
    "                print(deprobx, n_features, seed)\n",
    "                for method in tqdm(list_methods):\n",
    "                    if method == \"all\":\n",
    "                        feats = df_feature_ranks[f'triku'].sort_values().index[:]\n",
    "                    elif method == \"random\":\n",
    "                        array_selection = np.array([False] * len(df_feature_ranks))\n",
    "                        array_selection[np.random.choice(np.arange(len(df_feature_ranks)), n_features, replace=False)] = True\n",
    "                        \n",
    "                        feats = df_feature_ranks[f'triku'].sort_values().index[array_selection]\n",
    "                    else:\n",
    "                        feats = df_feature_ranks[method].sort_values().index[:n_features]\n",
    "                    \n",
    "                    adata_groups = [i.replace('Group', '') for i in adata_all.obs['Group']]\n",
    "                    c_f, res = clustering_binary_search(adata_all, min_res, max_res, max_depth, seed, len(list(dict.fromkeys(adata_groups))), feats, apply_log=False)\n",
    "                    NMS = NMI(c_f, adata_groups)\n",
    "                    ARS = ARI(c_f, adata_groups)\n",
    "\n",
    "                    df_NMI.loc[f'seed_{seed}', method] = NMS\n",
    "                    df_ARI.loc[f'seed_{seed}', method] = ARS\n",
    "            \n",
    "            print(os.getcwd() + f'/exports/comparisons/NMI_scatter_{deprobx}_n_features_{n_features}.csv')\n",
    "            df_NMI.to_csv(os.getcwd() + f'/exports/comparisons/NMI_scatter_{deprobx}_n_features_{n_features}.csv')\n",
    "            df_ARI.to_csv(os.getcwd() + f'/exports/comparisons/ARI_scatter_{deprobx}_n_features_{n_features}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.init(num_cpus=min(os.cpu_count(), len(list_deprobs)), ignore_reinit_error=True)\n",
    "ray_get = ray.get([leiden_adata_NMI_ARI.remote(deprobx) for deprobx in list_deprobs])\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(plot_lab_org_comparison_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_feats in ['250', '500']:\n",
    "    list_files = [f'NMI_scatter_{deprob}_n_features_{n_feats}.csv' for deprob in list_deprobs[::-1]]\n",
    "    fig = plot_lab_org_comparison_scores(f'NMI-{n_feats}', '', save_dir, [''], increasing=0, mode='ARI', list_files=list_files, \n",
    "                                       title=f'NMI on artificial datasets, {n_feats} features', \n",
    "                                       filename=f'NMI_{n_feats}-features', do_return=True, FS_methods=list_methods_all, palette=palette_all)\n",
    "    _ = fig.axes[1].set_xticklabels([item.get_text().replace('scatter ', '') for item in fig.axes[1].get_xticklabels()], rotation=0, ha='center')\n",
    "    for fmt in [\"png\", \"pdf\"]:\n",
    "        fig.savefig(f\"{os.getcwd()}/figures/comparison_figs/{fmt}/NMI_{n_feats}-features.{fmt}\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_feats in ['250', '500']:\n",
    "    list_files = [f'ARI_scatter_{deprob}_n_features_{n_feats}.csv' for deprob in list_deprobs[::-1]]\n",
    "    fig = plot_lab_org_comparison_scores(f'ARI-{n_feats}', '', save_dir, [''], increasing=0, mode='ARI', list_files=list_files, \n",
    "                                       title=f'ARI on artificial datasets, {n_feats} features', \n",
    "                                       filename=f'ARI_{n_feats}-features', do_return=True, FS_methods=list_methods_all, palette=palette_all)\n",
    "    _ = fig.axes[1].set_xticklabels([item.get_text().replace('scatter ', '') for item in fig.axes[1].get_xticklabels()], rotation=0, ha='center')\n",
    "    for fmt in [\"png\", \"pdf\"]:\n",
    "        fig.savefig(f\"{os.getcwd()}/figures/comparison_figs/{fmt}/ARI_{n_feats}-features.{fmt}\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For lower number of features (250) scanpy performs best at lower DE probabilities (up to 0.025) but performs worse at full resolution (0.1 or 0.3), with scry the best method for that, principally because the features that make smaller clusters separate are the ones with most expression, and those are the ones selected by scry. However, at smaller DE probabilities, the features that separate the dataset the most are the ones with mid expression levels, which are best picked by triku. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_values = pd.read_csv(f'{save_dir}/scatter_0.01_feature_values.csv', index_col=0)\n",
    "df_ranks = pd.read_csv(f'{save_dir}/scatter_0.01_feature_ranks.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read(splatter_dir + f'/splatter_deprob_0.01.loom', sparse=False)\n",
    "sc.pp.subsample(adata, 0.4) # We shorthen this to make the calculations not take 8 hours!\n",
    "\n",
    "adata.raw = adata\n",
    "sc.pp.pca(adata)\n",
    "sc.pp.neighbors(adata)\n",
    "sc.tl.umap(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bokeh = pd.DataFrame({'m': np.log10(adata.X.mean(0)), \n",
    "                         'z': df_values['triku'].loc[adata.var_names].values, \n",
    "                         'n': df_values.index.values})\n",
    "                   \n",
    "p = figure(tools=\"box_zoom,hover,reset\", plot_height=600, plot_width=600, tooltips=[(\"Gene\",\"@n\")])\n",
    "p.scatter('m', 'z', source=df_bokeh, alpha=0.7, line_color=None)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triku_hvg = df_ranks['triku'].values < 250\n",
    "scry_hvg = df_ranks['scry'].values < 250\n",
    "std_hvg = df_ranks['std'].values < 250\n",
    "scanpy_hvg = df_ranks['scanpy'].values < 250\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(12, 3))\n",
    "names = ['triku', 'scanpy', 'std', 'scry',]\n",
    "hvgs = [triku_hvg, scanpy_hvg, std_hvg, scry_hvg]\n",
    "colors = ['#e73f74', '#7f3c8d', '#11a579', '#3969ac']\n",
    "\n",
    "for i in range(4):\n",
    "    axs[i].scatter(np.log10(adata.X.mean(0))[~hvgs[i]][::5], \n",
    "                   df_values['triku'].loc[adata.var_names].values[~hvgs[i]][::5], c=\"#dedede\", s=2, alpha=0.7)\n",
    "    axs[i].scatter(np.log10(adata.X.mean(0))[hvgs[i]], \n",
    "                   df_values['triku'].loc[adata.var_names].values[hvgs[i]], c=colors[i], s=2, label=names[i])\n",
    "    axs[i].legend()\n",
    "\n",
    "fig.text(0.0, 0.5, 'Wasserstein distance', va='center', rotation='vertical')\n",
    "fig.text(0.5, 0.0, 'log$_{10}$ mean expression', va='center', rotation='horizontal')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.getcwd() + f'/figures/comparison_figs/pdf/barplots_scatter.pdf', fmt='pdf')\n",
    "\n",
    "\n",
    "list_list_genes = [['Gene1118', 'Gene8599', 'Gene1513', 'Gene1479'],     # triku only\n",
    "                   ['Gene6723', 'Gene6625', 'Gene9796', 'Gene935'],      # triku + scanpy\n",
    "                   ['Gene12841', 'Gene10739', 'Gene6729', 'Gene12240'],  # scanpy\n",
    "                   ['Gene9545', 'Gene4459', 'Gene383', 'Gene12455'],     # all\n",
    "                   ['Gene1633', 'Gene10792', 'Gene2496', 'Gene12497']]   # std + scry\n",
    "\n",
    "list_bar_colors = ['#94346E', '#E17C05', '#0F8554', '#1D6996']\n",
    "\n",
    "for lg_idx, list_genes in enumerate(list_list_genes):\n",
    "    fig, axs = plt.subplots(1, 5, figsize=(3*5, 3))\n",
    "    \n",
    "    hvg = np.isin(adata.var_names, list_genes)\n",
    "    axs[0].scatter(np.log10(adata.X.mean(0))[~hvg][::5], \n",
    "                   df_values['triku'].loc[adata.var_names].values[~hvg][::5], c=\"#bcbcbc\", s=2, alpha=0.7)\n",
    "    \n",
    "    for i in range(1, 5):\n",
    "        for group in range(10):\n",
    "            axs[0].scatter(np.log10(adata.X.mean(0))[np.isin(adata.var_names, list_genes[i - 1])][::5], \n",
    "                           df_values['triku'].loc[adata.var_names].values[np.isin(adata.var_names, list_genes[i - 1])][::5], c=list_bar_colors[i - 1], s=7)\n",
    "\n",
    "                \n",
    "            data_values = adata[adata.obs['Group'] == 'Group' + str(group + 1)].X[:, np.argwhere(adata.var_names == list_genes[i-1])[0]].flatten()\n",
    "            mean, std = np.mean(data_values), np.std(data_values)\n",
    "\n",
    "            axs[i].bar(group + 1, mean, color=list_bar_colors[i - 1])\n",
    "            \n",
    "    axs[0].set_ylabel('Wasserstein distance')\n",
    "    axs[0].set_xlabel('log$_{10}$ mean expression')\n",
    "    axs[1].set_ylabel('Mean group expression')\n",
    "    axs[1].set_xlabel('Group')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(os.getcwd() + f'/figures/comparison_figs/pdf/barplots_{lg_idx}.pdf', fmt='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ding et al. / Mereu et al. datasets\n",
    "Now that we have seen that triku outperforms other methods in artificial datasets, at least when there is intrinsic noisiness, we are going to apply similar metrics to biological datasets. We are first going to use Mereu's and Ding's human + mouse benchmarking datasets. They will help us see biases on performance of all the methods, and also it will act as a validation of the results from the original papers.\n",
    "\n",
    "In this part, due to the large amount of datasets, and also due to the heterogeneity of genes, we will not apply use different number of features. Instead, we will run triku with seed 0, and select the default number of features that is automatically generated to select the features on the rest of methods. This will mean that different datasets will have different number of features, although each dataset will have the same number of features across methods. \n",
    "\n",
    "The two main methods that we will use to evaluate the feature selection are NMI and Silhouette scores.\n",
    "* NMI uses the assigned cell types from the paper (Mereu et al. use MatchSCore2 and Ding et al. uses a custom algorithm) and applies the same binary search for resolution.\n",
    "* Silhouette. It is used in two forms:\n",
    "    * Apply the same resolution to all datasets and all methods using the binary search, and get the Silhouette from there.\n",
    "    * Apply Silhouette to the benchmark-assigned cell types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature ranking dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This process takes ~3 hours (> 12 with scmer)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.getcwd() + '/exports/comparisons/'\n",
    "mereu_dir = os.getcwd() + '/data/Mereu_2020/'\n",
    "ding_dir = os.getcwd() + '/data/Ding_2020/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for libprep in tqdm(['10X', 'CELseq2', 'ddSEQ', 'Dropseq', 'inDrop', 'QUARTZseq', 'SingleNuclei', 'SMARTseq2']):\n",
    "    for org in ['human', 'mouse']:\n",
    "        print(libprep, org)\n",
    "        if os.path.exists(save_dir + f'mereu_{libprep}_{org}-log_feature_values.csv'):\n",
    "            print(f'{libprep}, {org} exists!')\n",
    "        else:\n",
    "            if not os.path.exists(mereu_dir + f'{libprep}_{org}.h5ad'):\n",
    "                print(print(libprep, org, 'is not available'))\n",
    "            else:\n",
    "                adata_libprep = sc.read(mereu_dir + f'{libprep}_{org}.h5ad')\n",
    "                create_df_feature_ranking(adata_libprep, f'mereu_{libprep}_{org}-log', apply_log=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for libprep in tqdm(['10X', 'CELseq2', 'Dropseq', 'inDrop', 'SMARTseq2', 'SingleNuclei', 'sci-RNA-seq', 'Seq-Well',  ]):\n",
    "    for org in ['human', 'mouse']:\n",
    "        print(libprep, org)\n",
    "        if os.path.exists(ding_dir + f'{libprep}_{org}.h5ad'):\n",
    "            if os.path.exists(save_dir + f'ding_{libprep}_{org}-log_feature_values.csv'):\n",
    "                print(f'{libprep}, {org} exists!')\n",
    "            else:\n",
    "                if not os.path.exists(ding_dir + f'{libprep}_{org}.h5ad'):\n",
    "                    print(print(libprep, org, 'is not available'))\n",
    "                else:\n",
    "                    adata_libprep = sc.read(ding_dir + f'{libprep}_{org}.h5ad')\n",
    "                    create_df_feature_ranking(adata_libprep, f'ding_{libprep}_{org}-log', apply_log=True)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def run_ARI_silhouette_rem(lib_prep, org, seed, lab, adata_dir, save_dir):\n",
    "    if os.path.exists(adata_dir + f'{lib_prep}_{org}.h5ad'):\n",
    "        if os.path.exists(save_dir + f'{lab}_{lib_prep}-log_{org}_comparison-scores_seed-{seed}.csv'):\n",
    "            print(f'{lib_prep}, {org}, {seed} exists!')\n",
    "        else:\n",
    "            adata = sc.read_h5ad(adata_dir + f'{lib_prep}_{org}.h5ad')\n",
    "            print(adata)\n",
    "            cell_type = 'cell_types' if 'cell_types' in adata.obs else 'CellType' # Somwhere I've fucked up with column name. Don't care where honestly.\n",
    "            df_rank = pd.read_csv(os.getcwd() + f'/exports/comparisons/{lab}_{lib_prep}_{org}-log_feature_ranks.csv', index_col=0)\n",
    "\n",
    "            biological_silhouette_ARI_table(adata, df_rank, outdir=save_dir, file_root=f'{lab}_{lib_prep}_{org}-log', seed=seed, \n",
    "                                                        cell_types_col=cell_type, n_procs=1)   \n",
    "    else:\n",
    "        print(adata_dir + f'{lib_prep}_{org}.h5ad does not exist!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mereu's datasets\n",
    "save_dir = os.getcwd() + '/exports/comparisons/'\n",
    "adata_dir = data_dir + 'Mereu_2020/'\n",
    "\n",
    "\n",
    "lib_preps = ['SingleNuclei', 'Dropseq', 'inDrop', '10X', 'SMARTseq2', 'CELseq2', 'QUARTZseq', 'ddSEQ'] \n",
    "orgs = ['mouse', 'human'] \n",
    "result = list(product(*[lib_preps, orgs, range(5)]))\n",
    "\n",
    "ray.init(ignore_reinit_error=True, num_cpus=min(len(result), os.cpu_count()))\n",
    "\n",
    "list_id = [run_ARI_silhouette_rem.remote(lib_prep, org, seed, 'mereu', adata_dir, save_dir) for lib_prep, org, seed in result]\n",
    "list_results = ray.get(list_id)\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ding's datasets\n",
    "save_dir = os.getcwd() + '/exports/comparisons/'\n",
    "adata_dir = data_dir + 'Ding_2020/'\n",
    "\n",
    "\n",
    "lib_preps = ['10X', 'CELseq2', 'Dropseq', 'inDrop', 'sci-RNAseq', 'Seq-Well', 'SingleNuclei', 'SMARTseq2']\n",
    "orgs = ['mouse', 'human'] \n",
    "result = list(product(*[lib_preps, orgs, range(5)]))\n",
    "\n",
    "ray.init(ignore_reinit_error=True, num_cpus=min(len(result), os.cpu_count()))\n",
    "\n",
    "list_id = [run_ARI_silhouette_rem.remote(lib_prep, org, seed, 'ding', adata_dir, save_dir) for lib_prep, org, seed in result]\n",
    "list_results = ray.get(list_id)\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 4A and 5A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lab in ['ding', 'mereu']:\n",
    "    save_dir = os.getcwd() + '/exports/comparisons/'\n",
    "    fig = plot_lab_org_comparison_scores(lab, org='-log', read_dir=save_dir, variables=['NMI'], figsize=(16, 4), title=f'NMI on {lab} datasets (log)', \n",
    "                                  filename=f'{lab}-NMI-log', do_return=True, sort_values='descending', FS_methods=list_methods_all, palette=palette_all)\n",
    "    _ = fig.axes[1].set_xticklabels([item.get_text().replace('-log', '') for item in fig.axes[1].get_xticklabels()])\n",
    "    for fmt in [\"png\", \"pdf\"]:\n",
    "        fig.savefig(f\"{os.getcwd()}/figures/comparison_figs/{fmt}/{lab}-NMI-log.{fmt}\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure S3A and S4A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lab in ['ding', 'mereu']:\n",
    "    save_dir = os.getcwd() + '/exports/comparisons/'\n",
    "    fig = plot_lab_org_comparison_scores(lab, org='-log', read_dir=save_dir, variables=['ARI'], figsize=(16, 4), title=f'ARI on {lab} datasets (log)', \n",
    "                                  filename=f'{lab}-ARI-log', do_return=True, sort_values='descending', FS_methods=list_methods_all, palette=palette_all)\n",
    "    _ = fig.axes[1].set_xticklabels([item.get_text().replace('-log', '') for item in fig.axes[1].get_xticklabels()])\n",
    "    for fmt in [\"png\", \"pdf\"]:\n",
    "        fig.savefig(f\"{os.getcwd()}/figures/comparison_figs/{fmt}/{lab}-ARI-log.{fmt}\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 4B,5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lab in ['ding', 'mereu']:\n",
    "    save_dir = os.getcwd() + '/exports/comparisons/'\n",
    "    fig = plot_lab_org_comparison_scores(lab, org='-log', read_dir=save_dir, variables=['Sil_bench_all_hvg'], figsize=(16, 4), \n",
    "                                       title=f'Silhouette on {lab} datasets, cell types on selected features (log)',\n",
    "                                       filename=f'{lab}-silhouette_selected features_celltypes-log', do_return=True, sort_values='descending', \n",
    "                                         FS_methods=list_methods_all, palette=palette_all)\n",
    "    _ = fig.axes[1].set_xticklabels([item.get_text().replace('-log', '') for item in fig.axes[1].get_xticklabels()])\n",
    "    for fmt in [\"png\", \"pdf\"]:\n",
    "        fig.savefig(f\"{os.getcwd()}/figures/comparison_figs/{fmt}/{lab}-silhouette_selected features_celltypes-log.{fmt}\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure S3B and S4B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lab in ['ding', 'mereu']:\n",
    "    save_dir = os.getcwd() + '/exports/comparisons/'\n",
    "    fig = plot_lab_org_comparison_scores(lab, org='-log', read_dir=save_dir, variables=['Sil_leiden_all_hvg'], figsize=(16, 4), \n",
    "                                   title=f'Silhouette on {lab} datasets, leiden clusters on selected features (log)', \n",
    "                                  filename=f'{lab}-silhouette_selected features_leiden-log', do_return=True, sort_values='descending', \n",
    "                                  FS_methods=list_methods_all, palette=palette_all)\n",
    "    _ = fig.axes[1].set_xticklabels([item.get_text().replace('-log', '') for item in fig.axes[1].get_xticklabels()])\n",
    "    for fmt in [\"png\", \"pdf\"]:\n",
    "        fig.savefig(f\"{os.getcwd()}/figures/comparison_figs/{fmt}/{lab}-silhouette_selected features_leiden-log.{fmt}\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability of results\n",
    "Although we see that triku has promising results, we were striked at how std, scry and brennecke have such a big gap of scores with respect to triku, nbumi and m3drop. In this section we are going to apply some comprobation measures to see if we can know why the difference is so big. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlap heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmaps_jaccard(df_ranks, n_HVG, fig_save_dir='', title='', ax=None, lab='ding'):\n",
    "    df_heatmap = pd.DataFrame(np.NaN, index=df_ranks.columns, columns=df_ranks.columns)\n",
    "    \n",
    "    for row_idx, row in enumerate(df_ranks.columns):\n",
    "        for col_idx, col in enumerate(df_ranks.columns):\n",
    "            if row_idx >= col_idx:\n",
    "                row_names = set(df_ranks.sort_values(by=row).index[:n_HVG].values)\n",
    "                col_names = set(df_ranks.sort_values(by=col).index[:n_HVG].values)\n",
    "                \n",
    "                jaccard = len(row_names & col_names)/len(row_names | col_names)\n",
    "                df_heatmap.loc[row, col] = jaccard\n",
    "    \n",
    "    h = sns.heatmap(df_heatmap, cbar=False, ax=ax, annot=True, fmt='.1g')\n",
    "    h.set_title(title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    for fmt in ['png', 'pdf']:\n",
    "        plt.savefig(f'{fig_save_dir}/{lab}_heatmap_overlap_features.{fmt}', bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(4.3*4, 4.3))\n",
    "\n",
    "list_files = ['ding_10X_mouse-log_comparison-scores', 'ding_Dropseq_human-log_comparison-scores',\n",
    "              'ding_Seq-Well_human-log_comparison-scores', ]\n",
    "\n",
    "for libprep_idx, libprep in enumerate(list_files):\n",
    "    pre = '' if libprep_idx == 0 else '_'\n",
    "    for method_idx, method in enumerate(list_methods_all):\n",
    "        list_y = []\n",
    "        for seed in range(5):\n",
    "            df = pd.read_csv(os.getcwd() + f'/exports/comparisons/{libprep}_seed-{seed}.csv', index_col=0)\n",
    "            list_y.append(df.loc['NMI', method])\n",
    "\n",
    "        axs[0].bar(\n",
    "            libprep_idx + (method_idx - len(list_methods_all) // 2) * 0.08,\n",
    "            np.mean(list_y), width=0.08, yerr=np.std(list_y), color=palette_all[method_idx], \n",
    "            label=pre + method,\n",
    "        )\n",
    "        \n",
    "axs[0].set_xticks([0, 1, 2])\n",
    "axs[0].set_xticklabels(['10X mouse', 'Dropseq human', 'Seq-Well human'], rotation=45, ha='right')\n",
    "axs[0].set_title('NMI on ding datasets')\n",
    "axs[0].legend(ncol=2, handleheight=0.3, labelspacing=0.05, prop={'size': 8}, frameon=False)\n",
    "axs[0].set(frame_on=False)\n",
    "\n",
    "# Next are the heatmaps of gene overlap\n",
    "df_ranks = pd.read_csv(os.getcwd() + '/exports/comparisons/ding_10X_mouse-log_feature_ranks.csv', index_col=0)\n",
    "df_ranks = df_ranks[list_methods]\n",
    "plot_heatmaps_jaccard(df_ranks, n_HVG=1623, fig_save_dir=os.getcwd() + '/figures/comparison_figs', \n",
    "                      title='10X mouse', ax=axs[1])\n",
    "\n",
    "df_ranks = pd.read_csv(os.getcwd() + '/exports/comparisons/ding_Dropseq_human-log_feature_ranks.csv', index_col=0)\n",
    "df_ranks = df_ranks[list_methods]\n",
    "plot_heatmaps_jaccard(df_ranks, n_HVG=1294, fig_save_dir=os.getcwd() + '/figures/comparison_figs', \n",
    "                      title='Dropseq human', ax=axs[2])\n",
    "\n",
    "df_ranks = pd.read_csv(os.getcwd() + '/exports/comparisons/ding_Seq-Well_human-log_feature_ranks.csv', index_col=0)\n",
    "df_ranks = df_ranks[list_methods]\n",
    "plot_heatmaps_jaccard(df_ranks, n_HVG=1256, fig_save_dir=os.getcwd() + '/figures/comparison_figs', \n",
    "                      title='Seq-Well human', ax=axs[3])\n",
    "\n",
    "fig.savefig(f\"{os.getcwd()}/figures/comparison_figs/pdf/ding_heatmap_overlap_features.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrichment of ribosomal and mitochondrial genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot_mt_rbp(lab, org, method, n_features=[100, 250, 500, 1000], mode=0):   \n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 6))\n",
    "    \n",
    "    for n_feature_idx, n_feature in enumerate(n_features):\n",
    "        for FS_idx, FS in enumerate(list_methods):\n",
    "            df = pd.read_csv(os.getcwd() + f'/exports/comparisons/{lab}_{method}_{org}-log_feature_ranks.csv', index_col=0)\n",
    "\n",
    "            set_rbp = set([i for i in df.index if (i.upper().startswith('RPS')) | (i.upper().startswith('RPL'))])\n",
    "            set_mt = set([i for i in df.index if (i.upper().startswith('MT-'))])\n",
    "            \n",
    "            set_FS = set(df.sort_values(by=FS).index.tolist()[:n_feature])\n",
    "            \n",
    "            if mode == 0:\n",
    "                axs[0].bar(n_feature_idx + (FS_idx - len(list_FS) // 2) / (len(list_FS) + 3) , 100 * len(set_rbp & set_FS)/len(set_FS), \n",
    "                        width = 0.1, color=palette_all[FS_idx])\n",
    "                axs[1].bar(n_feature_idx + (FS_idx - len(list_FS) // 2) / (len(list_FS) + 3) , 100 * len(set_mt & set_FS)/len(set_FS), \n",
    "                        width = 0.1, color=palette_all[FS_idx])\n",
    "            else:\n",
    "                axs[0].bar(n_feature_idx + (FS_idx - len(list_FS) // 2) / (len(list_FS) + 3) , 100 * len(set_rbp & set_FS)/len(set_rbp), \n",
    "                        width = 0.1, color=palette_all[FS_idx])\n",
    "                axs[1].bar(n_feature_idx + (FS_idx - len(list_FS) // 2) / (len(list_FS) + 3) , 100 * len(set_mt & set_FS)/len(set_rbp), \n",
    "                        width = 0.1, color=palette_all[FS_idx])\n",
    "                \n",
    "    for ax in axs:\n",
    "        ax.set_xticks(range(len(n_features)))\n",
    "        ax.set_xticklabels(n_features)\n",
    "    \n",
    "    if mode == 0:\n",
    "        axs[0].set_ylabel('% ribosomal genes\\n(from selected features)')\n",
    "        axs[1].set_ylabel('% mitochondrial genes\\n(from selected features)')\n",
    "    else:\n",
    "        axs[0].set_ylabel('% ribosomal genes\\n(from all ribosomal genes)')\n",
    "        axs[1].set_ylabel('% mitochondrial genes\\n(from all mitochondrial genes)')\n",
    "        \n",
    "    legend_elements = [mpl.lines.Line2D([0], [0], marker=\"o\", color=palette[0], label='triku')] + [\n",
    "        mpl.lines.Line2D(\n",
    "            [0], [0], marker=\"o\", color=palette_all[j], label=list_methods[j]\n",
    "        )\n",
    "        for j in range(1, len(list_methods))\n",
    "    ]\n",
    "    axs[0].legend(handles=legend_elements, bbox_to_anchor=(1.2, 0.9))\n",
    "    \n",
    "    \n",
    "def heatmap_mt_rbp(labs, orgs, methods, n_features=500):        \n",
    "    dict_info = {}\n",
    "    \n",
    "    for lab in labs:\n",
    "        for org in orgs:\n",
    "            for method in methods:\n",
    "                for FS_idx, FS in enumerate(list_methods):\n",
    "                    if not os.path.exists(os.getcwd() + f'/exports/comparisons/{lab}_{method}_{org}-log_feature_ranks.csv'):\n",
    "                        continue\n",
    "                        \n",
    "                    df = pd.read_csv(os.getcwd() + f'/exports/comparisons/{lab}_{method}_{org}-log_feature_ranks.csv', index_col=0)\n",
    "\n",
    "                    set_rbp = set([i for i in df.index if (i.upper().startswith('RPS')) | (i.upper().startswith('RPL'))])\n",
    "                    set_mt = set([i for i in df.index if (i.upper().startswith('MT-'))])\n",
    "\n",
    "                    set_FS = set(df.sort_values(by=FS).index.tolist()[:n_features])\n",
    "                    \n",
    "                    for opt in [f'{FS}_per_rbp_all_features', f'{FS}_per_mt_all_features',]:\n",
    "                        if opt not in dict_info:\n",
    "                            dict_info[opt] = []\n",
    "                    \n",
    "                    dict_info[f'{FS}_per_rbp_all_features'].append(100 * len(set_rbp & set_FS) / len(set_FS))\n",
    "                    dict_info[f'{FS}_per_mt_all_features'].append(100 * len(set_mt & set_FS) / len(set_FS))\n",
    "    \n",
    "    df = pd.DataFrame(index=list_methods, columns=[\n",
    "        'Percentage RBPs in selected features', 'Percentage MTs in selected features', ])\n",
    "    \n",
    "    for FS_idx, FS in enumerate(list_methods):\n",
    "        df.iloc[FS_idx, 0] = '%.3f' % np.nanmean(dict_info[f'{FS}_per_rbp_all_features'])\n",
    "        df.iloc[FS_idx, 1] = '%.3f' % np.nanmean(dict_info[f'{FS}_per_mt_all_features'])\n",
    "                                \n",
    "                                     \n",
    "    return df.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = heatmap_mt_rbp(['mereu'], ['human', 'mouse'], ['SingleNuclei', 'Dropseq', 'inDrop', '10X', 'SMARTseq2', \n",
    "                                              'CELseq2', 'QUARTZseq', 'sci-RNAseq', 'Seq-Well'], n_features=2000)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = heatmap_mt_rbp(['ding'], ['human', 'mouse'], ['SingleNuclei', 'Dropseq', 'inDrop', '10X', 'SMARTseq2', \n",
    "                                              'CELseq2', 'QUARTZseq', 'sci-RNAseq', 'Seq-Well'], n_features=2000)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gene ontology analysis\n",
    "\n",
    "To see which method is better, a possible idea is to run Enrichr with the selected features, and use it to compare the FS methods. If the ontologies from one method have better p-values/scores, it is likely that they are more representative of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gseapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.getcwd() + f'/exports/enrichr/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_onto_mouse = ['KEGG_2019_Mouse', 'WikiPathways_2019_Mouse', 'GO_Biological_Process_2018', 'GO_Cellular_Component_2018', \n",
    "#                    'GO_Molecular_Function_2018',]\n",
    "\n",
    "# list_onto_human = ['KEGG_2019_Human', 'WikiPathways_2019_Human', 'GO_Biological_Process_2018', 'GO_Cellular_Component_2018', \n",
    "#                    'GO_Molecular_Function_2018', ]\n",
    "\n",
    "\n",
    "list_onto_mouse = ['GO_Biological_Process_2018']\n",
    "list_onto_human = ['GO_Biological_Process_2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def call_enrichr(lab, org, method, n_features, FS):\n",
    "    if os.path.exists(os.getcwd() + f'/exports/enrichr/{lab}_{method}_{org}_{n_features}_{FS}.csv'):\n",
    "        print(os.getcwd() + f'/exports/enrichr/{lab}_{method}_{org}_{n_features}_{FS}.csv EXISTS!')\n",
    "        return None\n",
    "    \n",
    "    if not os.path.exists(os.getcwd() + f'/exports/comparisons/{lab}_{method}_{org}-log_feature_ranks.csv'):\n",
    "        return None\n",
    "    \n",
    "    df_file = pd.read_csv(os.getcwd() + f'/exports/comparisons/{lab}_{method}_{org}-log_feature_ranks.csv', index_col=0)\n",
    "    \n",
    "    list_genes = df_file.sort_values(by=FS).index.tolist()[:n_features]\n",
    "    list_onto = list_onto_mouse if org == 'mouse' else list_onto_human\n",
    "    \n",
    "    n_trials = 0\n",
    "    \n",
    "    while n_trials < 5:\n",
    "        try:\n",
    "            result_df = gseapy.enrichr(list_genes, list_onto, cutoff=1, organism=org).results\n",
    "            result_df.to_csv(os.getcwd() + f'/exports/enrichr/{lab}_{method}_{org}_{n_features}_{FS}.csv', index=None)\n",
    "            n_trials += 10\n",
    "        except:\n",
    "#             raise\n",
    "            print(f'TRIAL {n_trials}')\n",
    "            \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_comb = list(product(*[['ding', 'mereu'], \n",
    "                           ['human', 'mouse'], \n",
    "                           ['SingleNuclei', 'Dropseq', 'inDrop', '10X', 'SMARTseq2', 'CELseq2', 'QUARTZseq', 'sci-RNAseq', 'Seq-Well'], \n",
    "                           [100, 250, 500, 1000, 1250, 1500], \n",
    "                           list_methods]))\n",
    "\n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "list_id = [call_enrichr.remote(lab, org, method, n_features, FS) for lab, org, method, n_features, FS in list_comb]\n",
    "list_results = ray.get(list_id)\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enrichr_figs_dir = os.getcwd() + '/figures/enrichr_figs/'\n",
    "os.makedirs(enrichr_figs_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_enrichr(lab, org, method, n_features, list_FS, palette, n_ontologies=30, column_sort='Adjusted P-value', plot_type='bar', \n",
    "                    list_onto=['KEGG_2019_Mouse', 'WikiPathways_2019_Mouse', 'KEGG_2019_Human', 'WikiPathways_2019_Human',\n",
    "                               'GO_Biological_Process_2018', 'GO_Cellular_Component_2018', 'GO_Molecular_Function_2018',], save=True, ):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 3))\n",
    "    \n",
    "    dict_dfs = {}\n",
    "    \n",
    "    for n_feature_idx, n_feature in enumerate(n_features):\n",
    "        for FS_idx, FS in enumerate(list_FS):\n",
    "            df = pd.read_csv(os.getcwd() + f'/exports/enrichr/{lab}_{method}_{org}_{n_feature}_{FS}.csv')\n",
    "            df = df[df['Gene_set'].isin(list_onto)]\n",
    "            \n",
    "            if column_sort == 'Adjusted P-value':\n",
    "                df = df.sort_values(by=column_sort).iloc[:n_ontologies]\n",
    "                y_vals = df[column_sort].values\n",
    "                y_vals = - np.log10(y_vals)\n",
    "\n",
    "            elif column_sort == 'Combined Score':\n",
    "                df = df.sort_values(by=column_sort, ascending=False).iloc[:n_ontologies]\n",
    "                y_vals = df[column_sort].values\n",
    "            \n",
    "            elif column_sort == 'division':\n",
    "                table_vals = df['Overlap'].values\n",
    "                df['divided'] = [int(i.split('/')[0]) / int(i.split('/')[1]) for i in table_vals]\n",
    "                df = df.sort_values(by='divided', ascending=False).iloc[:n_ontologies]\n",
    "                y_vals = df['divided'].values\n",
    "                \n",
    "            \n",
    "            x_pos = n_feature_idx + (FS_idx - len(list_FS) // 2) / (len(list_FS) + 3)\n",
    "            \n",
    "            if plot_type == 'bar':\n",
    "                plt.bar(x_pos , np.mean(y_vals), \n",
    "                        width = 0.1, yerr=np.std(y_vals), color=palette[FS_idx])\n",
    "            elif plot_type == 'scatter':\n",
    "                plt.scatter([x_pos] * len(y_vals), y_vals, c=palette[FS_idx], alpha=0.8)\n",
    "            \n",
    "            dict_dfs[f'{n_feature}_{FS}'] = df\n",
    "    \n",
    "    legend_elements = [\n",
    "        mpl.lines.Line2D(\n",
    "            [0], [0], marker=\"o\", color=palette[j], label=list_FS[j]\n",
    "        )\n",
    "        for j in range(len(list_FS))\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, bbox_to_anchor=(1.2, 0.9))\n",
    "    ax.set_xticks(range(len(n_features)))\n",
    "    ax.set_xticklabels(n_features)\n",
    "    ax.set_ylabel(column_sort)\n",
    "    ax.set_xlabel('Number of features')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(save)\n",
    "    \n",
    "    return dict_dfs\n",
    "\n",
    "def barplot_ontologies_individual(df, axis=None, color=\"#ababab\", column='Adjusted P-value', ascending=False, log=True, y_text=''):\n",
    "    if axis is None:\n",
    "        fig, axis = plt.subplots(1, 1, figsize=(10, 7))\n",
    "    \n",
    "    vals = df.sort_values(by=column, ascending=ascending)[column].values\n",
    "    names = [i.split(' (')[0] for i in df.sort_values(by=column, ascending=ascending)['Term'].values]\n",
    "    names = [i[: 42] + '...' if len(i) > 42 else i for i in names]\n",
    "\n",
    "    if log:\n",
    "        vals = - np.log10(df.sort_values(by=column, ascending=ascending)[column].values)\n",
    "    \n",
    "    if column == 'Adjusted P-value':\n",
    "        if log:\n",
    "            axis.plot(-np.log10([0.05, 0.05]), [-1.5, len(names) + 0.5], c=\"#ababab\", alpha=0.8, linewidth=3, zorder=0)\n",
    "        else:\n",
    "            axis.plot([0.05, 0.05], [-1.5, len(names) + 0.5], c=\"#ababab\", alpha=0.8, linewidth=3, zorder=0)\n",
    "        \n",
    "    axis.barh(range(len(df)), vals, color=color, zorder=5, alpha=0.7)\n",
    "    \n",
    "    for y in range(len(df)):\n",
    "        axis.text(0.05 * np.max(axis.get_xlim()), y - 0.2, names[y], zorder=10, fontsize=12)\n",
    "        \n",
    "    axis.set_yticks([])\n",
    "    axis.spines['right'].set_visible(False)\n",
    "    axis.spines['top'].set_visible(False)\n",
    "\n",
    "    x_text = column if not log else column + ' (log)'\n",
    "    axis.set_xlabel(x_text)\n",
    "    axis.set_ylabel(y_text)\n",
    "    \n",
    "    return axis\n",
    "\n",
    "def barplot_ontologies_all(dict_dfs, n_features=1000, list_FSs=['triku', 'std', 'scry', 'scanpy', 'm3drop', 'nbumi'], \n",
    "                           list_colors=[\"#E73F74\", \"#11A579\",\"#3969AC\", \"#7F3C8D\", \"#80BA5A\",\"#E68310\"], figsize=(17, 17), save=''):\n",
    "    \n",
    "    mpl.rcParams.update({'font.size':17})\n",
    "    fig, axis = plt.subplots(3, 3, figsize=figsize)\n",
    "    \n",
    "    for i in range(len(list_FSs)):\n",
    "        barplot_ontologies_individual(dict_dfs[f'{n_features}_{list_FSs[i]}'], axis=axis.ravel()[i], \n",
    "                                      color=list_colors[i], column='Adjusted P-value', ascending=False, log=True, y_text=list_FSs[i])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(save)\n",
    "        \n",
    "    mpl.rcParams.update(mpl.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab, org, method, n_features = 'ding', 'human', 'Dropseq', [100, 250, 500, 1000, 1250, 1500]\n",
    "list_dfs_ding_human_dropseq = []\n",
    "for x in ['Adjusted P-value']:  # ['Combined Score', 'Adjusted P-value', 'division']:\n",
    "    dict_df = scatter_enrichr(lab, org, method, n_features, list_FS = list_methods, palette=palette, n_ontologies=25, column_sort=x, plot_type='scatter', \n",
    "                    list_onto=[ 'GO_Biological_Process_2018',], save=enrichr_figs_dir + f'scatter_{lab}_{org}_{method}_{x}.pdf')\n",
    "    list_dfs_ding_human_dropseq.append(dict_df)\n",
    "    \n",
    "barplot_ontologies_all(list_dfs_ding_human_dropseq[0], save=enrichr_figs_dir + f'barplots_{lab}_{org}_{method}_{x}.pdf', \n",
    "                      list_FSs=list_methods, list_colors=palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure S6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab, org, method, n_features = 'ding', 'human', '10X', [100, 250, 500, 1000, 1250, 1500]\n",
    "list_dfs_ding_human_dropseq = []\n",
    "for x in ['Adjusted P-value']:  # ['Combined Score', 'Adjusted P-value', 'division']:\n",
    "    dict_df = scatter_enrichr(lab, org, method, n_features, list_FS = list_methods, palette=palette, n_ontologies=25, column_sort=x, plot_type='scatter', \n",
    "                    list_onto=[ 'GO_Biological_Process_2018',], save=enrichr_figs_dir + f'scatter_{lab}_{org}_{method}_{x}.pdf')\n",
    "    list_dfs_ding_human_dropseq.append(dict_df)\n",
    "    \n",
    "barplot_ontologies_all(list_dfs_ding_human_dropseq[0], save=enrichr_figs_dir + f'barplots_{lab}_{org}_{method}_{x}.pdf', \n",
    "                      list_FSs=list_methods, list_colors=palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of expression per cluster\n",
    "In this section we are going to study the distribution of expression of HVG across clusters. We are expected that the features selected by better FS methods are specific to fewer clusters. On the other hand, *worse* features are expressed across more clusters. To study this effect, we distribute, for each gene, its expression to sum one, and see what percentage of the total expression is located within the most expressed cluster, the 2nd most expressed cluster, etc. We observe that HVG selected by triku are more biased to be expressed in fewer clusters, compared to the results of other FS methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rc('font', **{'size': 13})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_mean_per(matrix):\n",
    "    # Returns the mean counts per gene, and the proportion of zeros\n",
    "    n_reads_per_gene = matrix.sum(0).astype(int)\n",
    "    n_zeros = (matrix == 0).sum(0)\n",
    "\n",
    "    return n_reads_per_gene/matrix.shape[0], n_zeros/matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_exp_cluster(adata, gene):\n",
    "    expression_vals = adata[:, gene].X.ravel()\n",
    "    expression_vals /= np.sum(expression_vals)\n",
    "    exp_by_cluster = sorted([sum(expression_vals[adata.obs['leiden'] == str(i)]) for i \n",
    "                      in range(np.max(adata.obs['leiden'].astype(int)) + 1)])[::-1]\n",
    "    return(exp_by_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure S5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 4, figsize=(12, 12))\n",
    "combos = product(*[['mereu', 'ding'], ['human', 'mouse']])\n",
    "datasets = ['10X', 'SMARTseq2', 'CELseq2', 'inDrop']\n",
    "\n",
    "cutoff, res = 1750, 1.2\n",
    "\n",
    "for col_idx, combo in enumerate(list(combos)):\n",
    "    for row_idx, dataset in enumerate(list(datasets)):\n",
    "        print('|||', combo, dataset, '|||')\n",
    "        try:\n",
    "            adata = sc.read(os.getcwd() + f'/data/{combo[0].capitalize()}_2020/{dataset}_{combo[1]}.h5ad')\n",
    "            df_ranks = pd.read_csv(os.getcwd() + f'/exports/comparisons/{combo[0]}_{dataset}_{combo[1]}-log_feature_ranks.csv', index_col=0)\n",
    "        except:\n",
    "            print(f'Combo {combo} {dataset} does not exist!')\n",
    "            axs[row_idx, col_idx].axis('off')\n",
    "            continue\n",
    "        \n",
    "        label_pre = '' if col_idx == 0 and row_idx == 0 else '_'  # This is to make the legend only appear for \n",
    "        \n",
    "        \n",
    "        combined_names = np.intersect1d(df_ranks.index.values, adata.var_names)\n",
    "        adata = adata[:, combined_names]\n",
    "        df_ranks = df_ranks.loc[combined_names]\n",
    "\n",
    "        sc.pp.log1p(adata)\n",
    "        sc.pp.pca(adata, random_state=seed)\n",
    "        sc.pp.neighbors(adata, random_state=seed, knn=int(0.5 * (len(adata) ** 0.5)), metric='cosine')\n",
    "        sc.tl.umap(adata, random_state=seed)\n",
    "        sc.tl.leiden(adata, resolution=res, random_state=seed)\n",
    "\n",
    "        for col_rest_idx, col_rest in enumerate(list_methods_all):\n",
    "            list_mean_exp, list_p_zeros = return_mean_per(adata.X)\n",
    "            list_genes = adata.var_names\n",
    "\n",
    "            if col_rest not in ['all', 'random']:\n",
    "                list_genes = df_ranks[df_ranks[col_rest] < cutoff].index\n",
    "            elif col_rest == 'all':\n",
    "                list_genes = df_ranks.index\n",
    "            elif col_rest == 'random':\n",
    "                list_genes = np.random.choice(df_ranks.index, cutoff) \n",
    "\n",
    "            list_clust = []\n",
    "            for gene in list_genes:\n",
    "                exp_clust = get_norm_exp_cluster(adata, gene)\n",
    "                list_clust.append(exp_clust)\n",
    "\n",
    "            arr = np.array(list_clust)\n",
    "\n",
    "            axs[row_idx, col_idx].plot(np.arange(len(exp_clust)), 100 * np.mean(arr, 0), color=palette_all[col_rest_idx], \n",
    "                                       alpha=1, label=col_rest, zorder=len(list_methods_all) - col_rest_idx)\n",
    "            \n",
    "        \n",
    "        axs[row_idx, col_idx].set_xlim([0, min(len(exp_clust), 8)])\n",
    "        axs[row_idx, col_idx].set_xticks(np.arange(min(len(exp_clust), 8) + 1))\n",
    "        axs[row_idx, col_idx].set_xticklabels(np.arange(min(len(exp_clust), 8) + 1))\n",
    "        \n",
    "        if row_idx == 0:\n",
    "            axs[row_idx, col_idx].set_title(' '.join(combo).capitalize())\n",
    "        if col_idx == 0:\n",
    "            axs[row_idx, col_idx].set_ylabel(dataset)\n",
    "            \n",
    "            \n",
    "# add a big axis, hide frame\n",
    "fig.add_subplot(111, frameon=False)\n",
    "# hide tick and tick label of the big axis\n",
    "plt.tick_params(labelcolor='none', which='both', top=False, bottom=False, left=False, right=False)\n",
    "plt.xlabel(\"Cluster (most to least expressed)\")\n",
    "\n",
    "plt.ylabel(\"% of expression\")\n",
    "plt.gca().yaxis.set_label_position(\"right\")\n",
    "\n",
    "axs[0, 0].legend(ncol=5, frameon=False, bbox_to_anchor=(4, -3.9), labelspacing=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.getcwd() + f'/figures/comparison_figs/comparison_clusters_benchmarking.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:triku-notebooks]",
   "language": "python",
   "name": "conda-env-triku-notebooks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
